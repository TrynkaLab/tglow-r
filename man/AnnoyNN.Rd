% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/annoy.r
\name{AnnoyNN}
\alias{AnnoyNN}
\title{Annoy: approximate nearest neighbours oh yeah}
\usage{
AnnoyNN(
  data,
  query = data,
  metric = "euclidean",
  n.trees = 50,
  k,
  search.k = -1,
  include.distance = TRUE,
  index = NULL
)
}
\arguments{
\item{data}{Data to build the index with}

\item{query}{A set of data to be queried against data}

\item{metric}{Distance metric; can be one of "euclidean", "cosine", "manhattan",
"hamming"}

\item{n.trees}{More trees gives higher precision when querying}

\item{k}{Number of neighbors}

\item{search.k}{During the query it will inspect up to search_k nodes which
gives you a run-time tradeoff between better accuracy and speed}

\item{include.distance}{Include the corresponding distances}

\item{index}{optional index object, will be recomputed if not provided}
}
\description{
This code is directly lifted from seurat's annoy implementation here:
https://github.com/satijalab/seurat/blob/master/R/clustering.R
}
\details{
More detaills on knn implementations in this paper:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11014608/

The reason for using this over exact NN is the speed, 
but other flavours of approximimate NN are available
Run annoy
}
